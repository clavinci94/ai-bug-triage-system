# ðŸ¤– AI Bug Triage System

Dieses Projekt ist ein kleiner Prototyp, den ich gebaut habe, um zu zeigen, wie man mit **FastAPI** und etwas **Machine Learning** automatisch Software-Issues klassifizieren kann.  
Die Idee ist simpel: neue Tickets (z. B. von GitHub) kommen rein, und ein Modell schlÃ¤gt vor, ob es sich um einen Bug, ein Feature, ein Dokumentationsproblem oder eine Verbesserung handelt.  
So spart man Zeit beim manuellen Einsortieren und hat eine einheitlichere Struktur.

---

## Warum â€žBug Triageâ€œ?

Wer schon mal an einem Open-Source-Projekt oder in einem Entwicklungsteam gearbeitet hat, kennt das Problem:  
Es kommen jede Menge Issues rein â€“ manche sind echte Bugs, manche nur VorschlÃ¤ge, andere wieder Dokumentationsfehler.  
Das manuell zu sortieren ist mÃ¼hsam und kostet Zeit.  

Mein Ziel war es, diesen Prozess **teilweise zu automatisieren**:  
- Neue Issues importieren (z. B. direkt von GitHub).  
- Automatisch eine Kategorie vorschlagen.  
- Das Modell kann immer wieder neu trainiert werden, sobald es mehr Daten gibt.  

---

## Features

- **/health** â†’ einfacher Check, ob die API lÃ¤uft  
- **/issues/import** â†’ Issues aus einem GitHub-Repo holen (z. B. `tiangolo/fastapi`)  
- **/issues/classify** â†’ neues Issue klassifizieren lassen  
- **/issues/classify?save=true** â†’ das Issue gleichzeitig ins Dataset aufnehmen  
- **/issues/dataset** â†’ aktuellen Datensatz ansehen (`issues.csv`)  
- **/issues/retrain** â†’ Modell neu trainieren  

Die API ist mit Swagger dokumentiert und kann direkt im Browser unter `/docs` ausprobiert werden.

---

## Projektstruktur

```text
ai-bug-triage-system/
â”œâ”€â”€ backend/
â”‚   â””â”€â”€ core/
â”‚       â”œâ”€â”€ main.py            # Einstiegspunkt (FastAPI App)
â”‚       â”œâ”€â”€ config.py          # Einstellungen (env Variablen)
â”‚       â”œâ”€â”€ models/
â”‚       â”‚   â””â”€â”€ schemas.py     # Pydantic Schemas
â”‚       â”œâ”€â”€ routes/
â”‚       â”‚   â””â”€â”€ issues.py      # API Endpunkte
â”‚       â””â”€â”€ services/
â”‚           â”œâ”€â”€ github.py      # Import von GitHub Issues
â”‚           â””â”€â”€ classifier.py  # Klassifikations-Logik (ML)
â”œâ”€â”€ data/
â”‚   â””â”€â”€ issues.csv             # Trainingsdaten
â”œâ”€â”€ requirements.txt
â””â”€â”€ README.md


## Installation & Start

Voraussetzung: Python 3.10+  

```bash
git clone <REPO_URL>
cd ai-bug-triage-system

python -m venv .venv
source .venv/bin/activate   # macOS/Linux
# .venv\Scripts\activate    # Windows

pip install -r requirements.txt


Falls man private GitHub-Repos importieren mÃ¶chte, legt man eine .env Datei an mit:
GITHUB_TOKEN=ghp_...mein_token...
Starten des Servers:
python -m uvicorn backend.core.main:app --reload
Dann im Browser Ã¶ffnen:
ðŸ‘‰ http://127.0.0.1:8000/docs

Beispiel-Requests
Klassifizieren
curl -X POST "http://127.0.0.1:8000/issues/classify" \
  -H "Content-Type: application/json" \
  -d '{
        "title": "App crashes on upload",
        "body": "Uploading a PDF freezes the app and it crashes."
      }'
Antwort (Beispiel):
{
  "category": "bug",
  "confidence": 0.82,
  "rationale": "NaiveBayes auf issues.csv"
}
Machine Learning
Aktuell steckt hinter der Klassifizierung ein recht einfaches Modell:
Texte werden mit TF-IDF in Vektoren umgewandelt
Klassifikation mit einem Naive Bayes Modell
Trainingsdaten liegen in issues.csv
Das SchÃ¶ne: wenn neue Daten reinkommen, kann das Modell direkt per API neu trainiert werden.
SpÃ¤ter kÃ¶nnte man hier stÃ¤rkere Modelle einsetzen (z. B. BERT oder Sentence Transformers), aber fÃ¼r den ersten Prototyp reicht das vÃ¶llig.

Roadmap
Mehr Trainingsdaten sammeln
Modell vergleichen mit moderneren AnsÃ¤tzen (Transformers)
Dataset-Upload Ã¼ber API ermÃ¶glichen
Integration mit externen Tools (z. B. JIRA, Slack)
Docker-Setup + CI/CD

